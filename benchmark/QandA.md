## 概率论与数理统计
**Q. 为什么要引入随机变量？**
<br>概率论的核心就是研究事件的概率值，也就相当于研究事件X与概率Y的映射关系Y=f(X)。
但之前研究事件的概率，都是以描述的方式定义事件X和事件的概率Y, 而描述的方式显然无法找到这种映射关系f。
<br>而通过引入随机变量，就把事件定义成了一个数字化的变量X，同时把概率定义成变量Y，这样就通过两个数字化变量X,Y能够定义出两者之间的关系f(X)=Y，
也就可以使用任何数学工具来分析这种映射关系了，这就是随机变量引入的价值。

**Q. 既然概率论就是研究x事件与y概率之间的映射关系，那有哪些常见的映射关系？**
<br>针对事件X自变量，主要有2种类型：
- 离散型事件的自变量X：
- 连续型事件的自变量X:
<br>针对概率Y映射值，也对应2种类型：
- 如果X为离散型自变量，则Y为离散型
- 如果X为连续型自变量，则Y为？？？

**Q. 什么是条件概率，什么是联合概率，两者有什么区别？**
<br>概率P(AB)代表事件A和事件B同时发生的概率，而条件概率P(B|A)代表在A发生的前提下B发生的概率。
其中P(AB)是在整个样本空间进行统计，而P(B|A)是在A发生的样本空间进行统计，也就是样本空间更小。
也可以理解为P(B|A)是在A发生的样本空间里AB同时发生的概率，所以才有公式P(B|A)=P(AB)/P(A)
<br>例如抛两次硬币，事件A为第一次为正面，事件B为第二次为正面，则P(AB)代表两次都是正面的概率，
P(B|A)代表第一次为正面的前提下再抛一次还为正面的概率。可采用手算的方式分析出计算两种概率：？

<br>公式：

**Q. ？**

---
## 机器学习通用知识
**Q. 如何判断过拟合，如何避免过拟合？**
<div align=center><img src="https://github.com/ximitiejiang/machine_learning_algorithm/blob/master/demo/svm_c_sigma.png"/></div>
<br>什么叫过拟合，就是模型学习到的参数只能针对训练特征，而对测试特征却效果不好，也就是模型泛化能力不行。
比如上图第二排就是一个从欠拟合到过拟合的过程：sigma(0.1)较大时内层没有泛化预测到，sigma(0.05)时模型基本把数据全部分开，
sigma(0.01)较小时模型对训练集完全预测准确但对测试集却效果很差，模型过拟合了。

<br>判断过拟合的方法：
- 方式1：分别在训练集与测试集上进行精度计算，如果训练集精度很高，而测试集精度很低，说明过拟合。
- 方式2：
- 方式2：可先对特征降维到2个维度，然后绘制出分割面，看看是否有过拟合的情况。

<br>解决过拟合的方法：
- 获取更多数据
- 减少特征数量：此时模型复杂度会降低
- 增加多项式特征：此时在模型复杂度不变情况下，增加了高维特征，也就增加了高维可分的可能性，变相相当于模型复杂度降低。


---
## PART 1. knn classifier
**Q. knn的超参数k怎么选择？**
<br>最好选择k为奇数，防止投票产生平票
<br>k值太小不好，使用太少的样本判断，极端情况就是只用一个样本，则模型预测太容易受噪声干扰，也就是模型只对训练数据有效果，泛化能力不行，也就是过拟合了。
<br>k值太大也不好，极端情况就是k值等于整个训练样本数，此时模型基本不需要训练，会直接预测出整个样本的最多类，说明模型太简单了，欠拟合了。

**Q. knn算法有什么缺陷，如何改进？**
<br>knn缺点：算法需要计算测试样本与训练样本的每一个样本的距离，如果训练样本很大则计算非常耗时。
<br>改进方法：kd树，预先把计算结果进行存储，从而减少计算距离的次数。kd树具体的算法过程：
- 先把样本保存到一个二叉树中(循环按照某一特征维度进行分割，以该维中位数作为分割点，生成二叉)
  这里如何选择哪一维度进行分割，可以按顺序选择，但有可能特征分布不均匀，所以更好的是按方差大的维度来选择，
- 假设有一个目标点x要预测，则先从根节点出发，判断每一维度大小，
比节点小往左分支，比节点大往右分支，直到抵达一个叶节点，该叶节点就是当前最近点。
- 从该叶节点回退，在每个节点进行距离计算，如果比当前最近点更近，则更新最近点坐标，


---
## PART 2. logistic regression classifier
**Q. 逻辑回归是否适合多分类？**
<br>直接的逻辑回归不适合多分类，但有如下三个思路可对逻辑回归算法进行改造使之适合多分类
- 扩展逻辑回归的外延，把-log(p)的p定义成softmax的输出，就可以适合多分类了，也就相当与softmax reg
- 采用一对一的方法：每一个类别之间进行一对一的分类，产生(n_class-1)的阶乘个分类器，然后投票决定预测结果
- 采用一对多的方法


---
## PART 3. softmax regression classifier


---
## PART 4. perceptron classifier



---
## PART 5. svm classifier
**Q. svm是否适合部分线性可分特征？对算法有什么要求？**
<br>线性svm只适合线性可分特征，而对于线性不可分特征，需要引入松弛系数，使每个样本的函数间隔减去一个松弛系数epsiloni，也就是函数间隔可以小于1，使yi*(w*xi +b) >= 1 - epsiloni。
松弛系数的加入，意味着可以放弃线性不可分的点的精确分类，放弃也就意味着分隔面不必向这些点的方向去移动，从而可以得到更大的几何间隔。
训练结果最终只有不可分的点有不为0的松弛系数，而可分点的松弛系统等于0，松弛系数越大，点就离群越远。
<br>同时，对每个松弛系数，损失函数(min f(w,b))需要增加支付一个代价C*sum(epsiloni)，这里C为惩罚系数。
C越大则说明对目标函数惩罚越大，说明你越重视离群点，也越难获得可收敛的解。
合理的调试方式就是定义一个C，然后求解得到一个分类器看看，如果过拟合了，可以考虑减小C，放弃一些错分点。

**Q. svm是否适合非线性可分特征？对算法有什么要求？**
<br>线性svm只适合线性可分和部分线性可分，而对于非线性可分，则需要把特征升维，使在低维线性不可分的特征变为高维线性可分的特征。
从而可以用线性svm的理论对非线性可分特征进行分类。
<br>而要计算得到高维线性可分特征需要分两步：第一步找到特征从低维向高维的映射函数，第二步计算每两个样本的内积
(之所以要计算两个样本内积，是因为线性svm求解采用了对偶求解方法，其公式中包含两个样本的内积)。
但这里第一步找到的特征从低维到高维的映射函数往往形式很复杂，不仅寻找麻烦而且计算过程麻烦，但两个样本
的映射函数的内积函数却有可能很简单，所以为了简化运算，引入核函数K(xi, xj)就定义成两个样本特征升维并内积的映射函数。
这种核函数就是把两步走的过程集成在一步映射完成，并且获得了非常简单的函数表达式。
<br>已经有一些现成的核函数，形式简单的表示出了特征升维后两个样本内积的结果，比如高斯核函数，比如二项式核函数。
注意核函数的输出结果是一个标量。

**Q. svm是否适合多分类特征？对算法有什么要求？**
<br>无论线性svm还是非线性svm，都只适合二分类问题，如果特征是多分类问题，则需要改造：
- 采用一对一的方法(这也是libsvm采用的方法)
- 采用一对多的方法

**Q. svm如何选择核函数？**
<br>有两类核函数：
- 线性可分问题，采用线性核函数，也就是不用核函数，K(xi, xj) = <xi, xj> = np.dot(xi, xj)即直接内积计算
- 非线性可分问题，优先采用高斯核函数，因为他只有一个参数sigma，便于调参。多项式核函数参数有3个，较难调参。

**Q. 为什么高斯核函数的sigma越小，模型越复杂？**
<br>高斯核函数相当于

**Q. svm的超参数如何选择：C, sigma？**
<div align=center><img src="https://github.com/ximitiejiang/machine_learning_algorithm/blob/master/demo/svm_c_sigma.png"/></div>
<br>对于C，如第一排的图，可以先尝试一个比较小的C，此时模型对离群点不够重视，可能放弃了太多离群点。
然后加大C，此时对目标函数惩罚较大，也代表对离群点更重视，不希望放弃太多离群点。
如果继续加大C，此时模型收敛更难，也更可能因为复杂的模型导致过拟合。

<br>对于sigma，如第二排的图，越小表明模型越复杂，越有可能过拟合，越大则模型越简单。
sigma(0.1)较大时内层没有泛化预测到，sigma(0.05)时模型基本把数据全部分开，
sigma(0.01)较小时模型对训练集完全预测准确但对测试集却效果很差，模型过拟合了。

**Q. svm与logistic回归都可以做线性分类器，两者有什么区别？**
<br>通过样本个数与特征个数的关系，来选择合适的分类器：
- 对于特征个数n


---
## PART 6. naive bayes classifier
**Q. 朴素贝叶斯**


---
## PART 7. CART classifier





