## 概率论与数理统计
**Q. 为什么要引入随机变量？**
<br>概率论的核心就是研究事件的概率值，也就相当于研究事件X与概率Y的映射关系Y=f(X)。
但之前研究事件的概率，都是以描述的方式定义事件X和事件的概率Y, 而描述的方式显然无法找到这种映射关系f。
<br>而通过引入随机变量，就把事件定义成了一个数字化的变量X，同时把概率定义成变量Y，这样就通过两个数字化变量X,Y能够定义出两者之间的关系f(X)=Y，
也就可以使用任何数学工具来分析这种映射关系了，这就是随机变量引入的价值。

**Q. 什么是条件概率，什么是联合概率，两者有什么区别？**



---
## 机器学习通用知识
**Q. 如何判断过拟合，如何避免过拟合？**
判断过拟合的方法：
- 方式1：分别在训练集与测试集上进行精度计算，如果训练集精度很高，而测试集精度很低，说明过拟合。
- 方式2：
- 方式2：可先对特征降维到2个维度，然后绘制出分割面，看看是否有过拟合的情况。
<br>解决过拟合的方法：
- 获取更多数据
- 减少特征数量：此时模型复杂度会降低
- 增加多项式特征：此时在模型复杂度不变情况下，增加了高维特征，也就增加了高维可分的可能性，变相相当于模型复杂度降低。


---
## PART 1. knn classifier
**Q. knn的超参数k怎么选择？**
<br>最好选择奇数，防止投票产生平票

---
## PART 2. logistic regression classifier
**Q. 逻辑回归是否适合多分类？**
<br>直接的逻辑回归不适合多分类，但有如下三个思路可对逻辑回归算法进行改造使之适合多分类
- 扩展逻辑回归的外延，把-log(p)的p定义成softmax的输出，就可以适合多分类了，也就相当与softmax reg
- 采用一对一的方法：每一个类别之间进行一对一的分类，产生(n_class-1)的阶乘个分类器，然后投票决定预测结果
- 采用一对多的方法


---
## PART 3. softmax regression classifier


---
## PART 4. perceptron classifier



---
## PART 5. svm classifier
**Q. svm是否适合部分线性可分特征？对算法有什么要求？**
线性svm只适合线性可分特征，而对于线性不可分特征，需要引入松弛系数，使每个样本的函数间隔减去一个松弛系数epsiloni，也就是函数间隔可以小于1，使yi*(w*xi +b) >= 1 - epsiloni。
松弛系数的加入，意味着可以放弃线性不可分的点的精确分类，放弃也就意味着分隔面不必向这些点的方向去移动，从而可以得到更大的几何间隔。
训练结果最终只有不可分的点有不为0的松弛系数，而可分点的松弛系统等于0，松弛系数越大，点就离群越远。
<br>同时，对每个松弛系数，损失函数(min f(w,b))需要增加支付一个代价C*sum(epsiloni)，这里C为惩罚系数。
C越大则说明对目标函数惩罚越大，说明你越重视离群点，也越难获得可收敛的解。
合理的调试方式就是定义一个C，然后求解得到一个分类器看看，如果过拟合了，可以考虑减小C，放弃一些错分点。

**Q. svm是否适合非线性可分特征？对算法有什么要求？**
线性svm只适合线性可分和部分线性可分，而对于非线性可分，则需要把特征升维，使在低维线性不可分的特征变为高维线性可分的特征。
从而可以用线性svm的理论对非线性可分特征进行分类。
<br>而要计算得到高维线性可分特征需要分两步：第一步找到特征从低维向高维的映射函数，第二步计算每两个样本的内积
(之所以要计算两个样本内积，是因为线性svm求解采用了对偶求解方法，其公式中包含两个样本的内积)。
但这里第一步找到的特征从低维到高维的映射函数往往形式很复杂，不仅寻找麻烦而且计算过程麻烦，但两个样本
的映射函数的内积函数却有可能很简单，所以为了简化运算，引入核函数K(xi, xj)就定义成两个样本特征升维并内积的映射函数。
这种核函数就是把两步走的过程集成在一步映射完成，并且获得了非常简单的函数表达式。
<br>已经有一些现成的核函数，形式简单的表示出了特征升维后两个样本内积的结果，比如高斯核函数，比如二项式核函数。
注意核函数的输出结果是一个标量。

**Q. svm是否适合多分类特征？对算法有什么要求？**
无论线性svm还是非线性svm，都只适合二分类问题，如果特征是多分类问题，则需要改造：
- 采用一对一的方法(这也是libsvm采用的方法)
- 采用一对多的方法

**Q. svm如何选择核函数？**
线性可分问题，采用线性核函数，也就是不用核函数，K(xi, xj) = <xi, xj> = np.dot(xi, xj)即直接内积计算
<br>非线性可分问题，优先采用高斯核函数，因为他只有一个参数sigma，便于调参。多项式核函数参数有3个，较难调参。

**Q. 为什么高斯核函数的sigma越小，模型越复杂？**
高斯核函数相当于

**Q. svm的超参数如何选择：C/sigma？**
对于C，可以先尝试一个比较大的C，此时对目标函数惩罚较大，也代表对离群点比较重视，模型收敛更难，也更可能因为复杂的模型导致过拟合。
如果结果有过拟合，则考虑减小C，再进行训练。
<br>对伊sigma

**Q. svm与logistic回归都可以做线性分类器，两者有什么区别？**
- 对于特征个数n


---
## PART 6. naive bayes classifier
**Q. 朴素贝叶斯**


---
## PART 7. CART classifier





