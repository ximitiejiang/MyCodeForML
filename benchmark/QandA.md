# 机器学习问题集 v1.0

## 概率论
**Q. 为什么要引入随机变量？**
<br>概率论的核心就是研究事件的概率值，也就相当于研究事件X与概率Y的映射关系Y=f(X)。
但之前研究事件的概率，都是以描述的方式定义事件X和事件的概率Y, 而这种描述的方式显然无法找到这种映射关系f。
<br>而通过引入随机变量，就把事件定义成了一个数字化的变量X，同时把概率定义成变量Y，这样就通过两个数字化变量X,Y能够定义出两者之间的关系f(X)=Y，
也就可以使用任何数学工具来分析这种映射关系了，这就是随机变量引入的价值。
<br>所以概率论就是在已知X事件的分布情况下，分析映射关系(分布率/概率密度/概率)，分析数字特征(期望，方差，相关系数，矩)

**Q. 既然概率论就是研究x事件与y概率之间的映射关系，那有哪些常见的映射关系？**
<br>针对事件X自变量，主要有2种类型：
- 离散型事件的自变量X：
- 连续型事件的自变量X:
<br>针对概率Y映射值，也对应2种类型：
- 如果X为离散型自变量，则Y为离散型
- 如果X为连续型自变量，则Y为？？？

<br>两种类型X/Y对应了两种数学工具来研究：分别定义(分布律或概率密度，以及分布函数)，就可以用数学方法求概率了。
- 离散型X,Y：X的分布律    ->  X的分布函数F(X)=P(X<x)=求和(X=xi)  ->   从而可求X的区间概率P(x1<x<x2)=F(x2) - F(x1)
- 连续型X,Y：X的概率密度f ->  X的分布函数F(X)=P(X<x)=积分f(t)dt  ->   从而可求X的区间概率P(x1<x<x2)=从x1到x2积分f(t)dt

**Q. 什么是条件概率P(B|A)，跟同时发生的概率P(AB)有什么区别？**
<br>概率P(AB)代表事件A和事件B同时发生的概率，而条件概率P(B|A)代表在A发生的前提下B发生的概率。
其中P(AB)是在整个样本空间进行统计，而P(B|A)是在A发生的样本空间进行统计，也就是样本空间更小。
也可以理解为P(B|A)是在A发生的样本空间里AB同时发生的概率，所以才有公式P(B|A)=P(AB)/P(A)

<br>例如抛两次硬币，事件A为第一次为正面，事件B为第二次为正面，则P(AB)代表两次都是正面的概率，
P(B|A)代表第一次为正面的前提下再抛一次还为正面的概率。可采用手算的方式分析出计算两种概率：
- P(AB)两次都是正面的概率：(11)/(11+10+01+00) = 1/4
- P(B|A)在第一次为正面条件下，第二次为正面的概率：1/2
- 而用条件概率公式P(B|A) = (1/4)/(1/2) = 1/2

<br>针对同时发生概率P(AB)的定义，有一个乘法公式：
- 如果两个事件相互独立，则P(AB)=P(A)P(B)
- 如果两个时间不独立，则P(AB)=P(B|A)P(A)，这就是乘法公式

<br>针对条件概率定义，有一个最有名公式，叫贝叶斯公式，这也是机器学习贝叶斯分类算法的基础。
- 全概率公式: P(A) = P(A|B1)P(B1) + P(A|B1)P(B1) + ... + P(A|Bn)P(Bn)，
即把样本空间分割，在每一个子样本空间求概率，再累加。比如抽样1个的次品概率，原样本空间是所有样品，可以分解成来自B1厂/B2厂/B3厂样本
的三个子样本空间，所以P(A) = P(A|B1)P(B1) + P(A|B2)P(B2) + P(A|B3)P(B3), 参考概率论与数理统计p19

- 贝叶斯公式: P(B|A) = P(A|B)P(B)/P(A), 这里分母P(A)还可以用全概率公式展开。
即把两个事件A和B进行反转来作为先验概率，A事件作为先验概率下B事件的概率，就转化成了B事件作为先验概率下A事件的概率

**Q. 对随机变量的数字特征的本质认识**
<br>期望和方差是用来描述概率论中总体的数字特征，所以求解时：
- 期望：即E(X)，描述的是数值的均值(也就是数值)。离散均值E(X)=sum(xk*pk)，连续均值E(X)=全积分(xf(x)dx)
- 方差：即D(X)，描述的是偏差的平均(也就是偏差)。即sigma^2。方差是数据偏离均值的距离平方的均值，也就是D(X)=E((X-E(x))^2)，平方是为了放大偏离程度，所以方差也是一种相对于均值的偏离量的均值的度量。
基于方差也是一种均值，所以求方差的公式就是用均值公式，只不过对象不是随机变量X，而是相对于均值的偏差平方(X-E(X))^2。
也就是说，离散方差D(X)=sum((xk-E(X))^2*pk)，连续方差D(X)=全积分((x-E(x))^2*f(x)dx)
- 方差计算1个重要公式：D(X)=E(X^2) - (E(X))^2 ，可用D(X)的定义展开即可证明。
- 标准差：F(X)，也就是sigma。

<br>如果是针对样本而不是总体，标准差公式是除以(n-1)，而总体的标准差公式是除以(n)


**Q. 偏差与方差的区别**
<br>偏差就是误差,也就是下图中的bias, 是相对于真值的距离，所以距离真值越远则偏差越大。
<br>方差就是均值，也就是下图中的Variance, 是相对于均值的距离，所以想对于均值越分散则方差越大。
<div align=center><img src="https://github.com/ximitiejiang/machine_learning_algorithm/blob/master/demo/bias_vs_variance.png"/></div>

**Q. 协方差与方差的区别**
- 方差是描述一个随机变量(也就是一个特征)不同取值的偏差情况sum(X-Xmean)^2/(n-1)，
而协方差是描述两个随机变量(也就是2个特征)之间的关系sum(X_Xmean)(Y-Ymean)/n-1
- 从公式可以看出，随机变量X跟自己的的协方差就是方差即cov(X,X)=var(X)，同时cov(X,Y)=cov(Y,X)
- 由于协方差矩阵只能描述2个特征之间的关系，那如果要描述n个特征之间的关系，就需要协方差矩阵：
```
C = (cov(X,X), cov(X,Y), cov(X,Z),
     cov(Y,X), cov(Y,Y), cov(Y,Z),
     cov(Z,X), cov(Z,Y), cov(Z,Z)) 
```
- 方差/协方差的意义：方差越大，就说明特征表达能力越强，极限情况特征所有值相同，此时方差=0，说明该特征完全没有意义。

---
### 统计学
**Q.概率论与数理统计的区别**
<br>概率论与数理统计的区别：(参考概率论与数理统计P128)
- 概率论就是：已知随机变量X的分布类型，来求1.分布律/概率密度/概率，2.数字特性(期望，方差，相关系数，矩)。
而且一般是先得到分布率/概率密度，才能再具体计算数字特性。
- 数理统计是：对随机变量X分布未知，也不知道分布律/概率密度，也不知道数字特征，
而是通过一部分样本数据，反过来先找数字特征，在分析分布律/概率密度，最后得知分布特点

**Q. 根据抽样计算抽样的数字特征**
<br>抽样的数字特征计算跟总体的特征计算类似，但有一个特殊的地方在样本方差
- 样本均值：Xmean = sum(Xi) / n
- 样本方差：S2 = sum((xi-Xmean)^2) / (n-1)
- 样本标准差：sqrt(sum((xi-Xmean)^2) / (n-1))

<br>在numpy中计算均值方差标准差，他并不能区分到底是总体还是抽样，但可以选择：
- 均值np.mean()，这条总体和样本计算是一样的。
- 标准差np.std()，默认是总体标准差，即ddof=0(是除以n)，也可设置ddof=1则为样本标准差(是除以n-1)(相当于放大一点标准差的值，更接近总体，也称无偏估计)
- 方差np.var()，默认是总体方差，即ddof=0(是除以n)，也可设置ddof=1为样本方差(是除以n-1)(相当于放大一点方差的值，更接近总体，也称无偏估计)

<br>为什么样本方差是除以n-1，且叫做无偏估计？
- 我理解的主要原因是对方差进行估计时，本应该是相对于总体均值的偏差平方的均值，
但通常总体均值未知而采用样本均值放入计算样本方差，此时计算出来的样本均值会比实际样本均值偏小，
从而造成低估方差，所以要把分母的n换成n-1来放大样本均值，从而可以抵消因为用样本均值替代总体均值造成的低估问题，从而得到无偏估计。
参考：https://blog.csdn.net/qq_39521554/article/details/79633207
证明：https://blog.csdn.net/Hearthougan/article/details/77859173 (应该是标准答案)

---
## 机器学习

**Q. 所有模型对比？**

|模型名称      |分类   |特征    |参数    |损失     |优点                              | 缺点                                               | 
|------------- |:-     |:-     |:-       |:-      |:-                               |:-                                                  |
|knn           |多分类 |非线性 |不可调参|不训练    |支持几乎所有类型数据，结构简单     |不是最优解，分隔面会侵入少样本区，计算量大              | 
|kdtree        |多分类 |非线性 |不可调参|不训练    |支持几乎所有类型数据，计算量有下降  |不是最优解，分隔面会侵入少样本区                       |
|logistic reg  |二分类 |线性   |不可调参|有损失函数|                                  |                                                      |
|softmax reg   |多分类 |线性   |不可调参|有损失函数|支持线性可分的多分类               |不是最优解，只支持线性分隔面                           |
|perceptron    |二分类 |线性   |不可调参|有损失函数|                                  |                                                      |
|svm           |多分类 |非线性 |可调参  |合页损失  |支持几乎所有类型数据，可参数优化   |完美!                                                  |
|naive bayes   |多分类 |非线性 |不可调参|不训练    |支持连续和离散数据                 |非线性能力不太强，对连续特征要求是正态分布否则分类效果差 |
|dt-cart       |多分类 |非线性 |不可调参|不训练    |                                  |                                                       |
|random forest |多分类 |非线性 |
|ada boost     |多分类 |非线性 |
|gradient boost|多分类|非线性 |
|xgboost       |多分类 |非线性 |
|mlp           |多分类 |非线性 |
|cnn           |多分类 |非线性 |


**Q. 当前模型精度对比？**
<br>数据集基于如下几个:
- iris: 简单的多特征多类别数据集，4个特征，3个类别
- digits: 复杂的多特征多类别数据集，1000多个特征，10个类别
- breast cancer: 复杂的多特征二分类数据集，30个特征，2个类别

<br>设置：采用7：3的训练测试集比例
<br>M表示多分类，2表示二分类

|模型名称       |特点                       |cancer(2)    |iris(M) |digits(M)| 
|------------- |:-                         |:-            |:-      |:-       |
|knn           |多分类/非线性可分           |              |0.985   |         |
|kdtree        |多分类/非线性可分           |              |        |         |
|logistic      |二分类/线性可分             |              |x       | x       |
|logistic ovo  |多分类/线性可分             |              |        |         |
|softmax       |多分类/线性可分             |              |0.967   |         |
|percptron     |二分类/线性可分             |              |x       | x       |
|percptron ovo |多分类/线性可分             |              |        |         |
|svmc          |二分类/非线性可分           |0.92          |x       | x       |
|svmc ovo      |多分类/非线性可分           |              |        |         |
|naive bayes   |多分类/非线性可分           |              |        |         |
|dt-cart       |多分类/非线性可分           |              |0.956   |         |
|random forest |多分类/非线性可分           |              |        |         |
|ada boost     |多分类/非线性可分           |              |        |         |
|gradient boost|多分类/非线性可分           |              |        |         |
|xgboost       |多分类/非线性可分           |              |        |         |
|mlp           |多分类/非线性可分           |              |        |         |
|cnn           |多分类/非线性可分           |              |        |         |


**Q. 如何判断过拟合，如何避免过拟合？**
<div align=center><img src="https://github.com/ximitiejiang/machine_learning_algorithm/blob/master/demo/svm_c_sigma.png"/></div>
<br>什么叫过拟合，就是模型学习到的参数只能针对训练特征，而对测试特征却效果不好，也就是模型泛化能力不行。
比如上图第二排就是一个从欠拟合到过拟合的过程：sigma(0.1)较大时内层没有泛化预测到，sigma(0.05)时模型基本把数据全部分开，
sigma(0.01)较小时模型对训练集完全预测准确但对测试集却效果很差，模型过拟合了。

<br>判断过拟合的方法：
- 方式1：分别在训练集与测试集上进行精度计算，如果训练集精度很高，而测试集精度很低，说明过拟合。
- 方式2：
- 方式2：可先对特征降维到2个维度，然后绘制出分割面，看看是否有过拟合的情况。

<br>解决过拟合的方法：
- 获取更多数据
- 减少特征数量：此时模型复杂度会降低
- 增加多项式特征：此时在模型复杂度不变情况下，增加了高维特征，也就增加了高维可分的可能性，变相相当于模型复杂度降低。


---
## PART 1. knn classifier
**Q. knn的超参数k怎么选择？**
<br>最好选择k为奇数，防止投票产生平票
<br>k值太小不好，使用太少的样本判断，极端情况就是只用一个样本，则模型预测太容易受噪声干扰，也就是模型只对训练数据有效果，泛化能力不行，也就是过拟合了。
<br>k值太大也不好，极端情况就是k值等于整个训练样本数，此时模型基本不需要训练，会直接预测出整个样本的最多类，说明模型太简单了，欠拟合了。

**Q. knn算法有什么缺陷，如何改进？**
<br>knn缺点：算法需要计算测试样本与训练样本的每一个样本的距离，如果训练样本很大则计算非常耗时。
<br>改进方法：kd树，预先把特征存放到二叉树中(kdtree)，从而在寻找k个样本时不需要跟整个样本空间的样本进行距离计算。
这在特征维度很高/样本数量很大时节省的时间就很多，不过在特征维度一般/样本数少的情况下，差别不大。
<br>kd树具体的算法过程：
- 先把样本保存到一个二叉树中(循环按照某一特征维度进行分割，以该维中位数作为分割点，生成二叉)
- 预测过程包括下行和上行两部分：下行就是从根节点出发，往下找到最接近测试样本的叶子结点(即左右子节点均为None的节点)。
而上行就是从该叶子节点往上，寻找k个点，逻辑是每往上爬一个节点就可以存放一个，如果超过k个则比较距离大小。
- 最后在找到的k个节点中，投票决定测试样本的类别。
- 可见kdtree算法也是找k个近邻来投票，跟knn区别在于先把特征保存成kdtree，然后搜索kdtree的方法比较高效。

- 假设有一个目标点x要预测，则先从根节点出发，判断每一维度大小，
比节点小往左分支，比节点大往右分支，直到抵达一个叶节点，该叶节点就是当前最近点。
- 从该叶节点回退，在每个节点进行距离计算，如果比当前最近点更近，则更新最近点坐标，


---
## PART 2. logistic regression classifier
**Q. 逻辑回归是否适合多分类？**
<br>直接的逻辑回归不适合多分类，但有如下三个思路可对逻辑回归算法进行改造使之适合多分类
- 扩展逻辑回归的外延，把-log(p)的p定义成softmax的输出，就可以适合多分类了，也就相当与softmax reg
- 采用一对一的方法：每一个类别之间进行一对一的分类，产生(n_class-1)的阶乘个分类器，然后投票决定预测结果
- 采用一对多的方法


---
## PART 3. softmax regression classifier


---
## PART 4. perceptron classifier



---
## PART 5. svm classifier
**Q. svm是否适合部分线性可分特征？对算法有什么要求？**
<br>线性svm只适合线性可分特征，而对于线性不可分特征，需要引入松弛系数，使每个样本的函数间隔减去一个松弛系数epsiloni，也就是函数间隔可以小于1，使yi*(w*xi +b) >= 1 - epsiloni。
松弛系数的加入，意味着可以放弃线性不可分的点的精确分类，放弃也就意味着分隔面不必向这些点的方向去移动，从而可以得到更大的几何间隔。
训练结果最终只有不可分的点有不为0的松弛系数，而可分点的松弛系统等于0，松弛系数越大，点就离群越远。
<br>同时，对每个松弛系数，损失函数(min f(w,b))需要增加支付一个代价C*sum(epsiloni)，这里C为惩罚系数。
C越大则说明对目标函数惩罚越大，说明你越重视离群点，也越难获得可收敛的解。
合理的调试方式就是定义一个C，然后求解得到一个分类器看看，如果过拟合了，可以考虑减小C，放弃一些错分点。

**Q. svm是否适合非线性可分特征？对算法有什么要求？**
<br>线性svm只适合线性可分和部分线性可分，而对于非线性可分，则需要把特征升维，使在低维线性不可分的特征变为高维线性可分的特征。
从而可以用线性svm的理论对非线性可分特征进行分类。
<br>而要计算得到高维线性可分特征需要分两步：第一步找到特征从低维向高维的映射函数，第二步计算每两个样本的内积
(之所以要计算两个样本内积，是因为线性svm求解采用了对偶求解方法，其公式中包含两个样本的内积)。
但这里第一步找到的特征从低维到高维的映射函数往往形式很复杂，不仅寻找麻烦而且计算过程麻烦，但两个样本
的映射函数的内积函数却有可能很简单，所以为了简化运算，引入核函数K(xi, xj)就定义成两个样本特征升维并内积的映射函数。
这种核函数就是把两步走的过程集成在一步映射完成，并且获得了非常简单的函数表达式。
<br>已经有一些现成的核函数，形式简单的表示出了特征升维后两个样本内积的结果，比如高斯核函数，比如二项式核函数。
注意核函数的输出结果是一个标量。

**Q. svm是否适合多分类特征？对算法有什么要求？**
<br>无论线性svm还是非线性svm，都只适合二分类问题，如果特征是多分类问题，则需要改造：
- 采用一对一的方法(这也是libsvm采用的方法)
- 采用一对多的方法

**Q. svm如何选择核函数？**
<br>有两类核函数：
- 线性可分问题，采用线性核函数，也就是不用核函数，K(xi, xj) = <xi, xj> = np.dot(xi, xj)即直接内积计算
- 非线性可分问题，优先采用高斯核函数，因为他只有一个参数sigma，便于调参。多项式核函数参数有3个，较难调参。

**Q. 为什么高斯核函数的sigma越小，模型越复杂？**
<br>高斯核函数相当于

**Q. svm的超参数如何选择：C, sigma？**
<div align=center><img src="https://github.com/ximitiejiang/machine_learning_algorithm/blob/master/demo/svm_c_sigma.png"/></div>
<br>对于C，如第一排的图，可以先尝试一个比较小的C，此时模型对离群点不够重视，可能放弃了太多离群点。
然后加大C，此时对目标函数惩罚较大，也代表对离群点更重视，不希望放弃太多离群点。
如果继续加大C，此时模型收敛更难，也更可能因为复杂的模型导致过拟合。

<br>对于sigma，如第二排的图，越小表明模型越复杂，越有可能过拟合，越大则模型越简单。
sigma(0.1)较大时内层没有泛化预测到，sigma(0.05)时模型基本把数据全部分开，
sigma(0.01)较小时模型对训练集完全预测准确但对测试集却效果很差，模型过拟合了。

**Q. svm在线性分类器及非线性分类器中有什么优势？**
<br>在线性分类器中，常用的是perceptron/logistic reg和svm，但perceptron采用的函数间隔(wx+b)来区分负样本，
也就无法对正负样本的相互距离关系进行评价；而svm采用的几何间隔，能够让正负样本的几何间隔都最大化，从而保证
正负样本的支持向量距离分隔面距离相同，属于最优化的分隔面。

<br>在非线性分类器中，常用的两种是knn和svm，但knn无法调参数，他对于局部样本不平衡的情况无法干预，
会导致分隔面往样本少的方向侵入，不是最优化的分隔方式；而svm由于在局部是采用最大化几何间隔来实现，
所以属于在最优化的分隔面，样本不平衡问题对svm没有影响。如下对比knn与svm的图片可以看出knn分隔面往样不少方向的入侵问题。
<div align=center><img src="https://github.com/ximitiejiang/machine_learning_algorithm/blob/master/demo/compoare_knn_svm.png"/></div>

---
## PART 6. naive bayes classifier
**Q. 朴素贝叶斯是否适合连续性特征的分类？**
<br>朴素贝叶斯可以适合于离散性特征和连续性特征，共同点都是基于条件独立性假设下的条件概率公式
P(Y=ck|X=x) = argmax P(Y=ck)*连乘积P(Xj=xj|Y=ck)来计算样本属于每个类的概率，其中概率最大的就是预测值。

<br>朴素贝叶斯分类方法在处理离散性特征和连续性特征的区别在于连乘积P(Xj=xj|Y=ck)的计算方式
- 对于离散性特征，采用统计个数的方式来计算在某个类别下，每个离散特征值的出现个数来计算概率。
- 对于连续性特征，是在条件独立性假设前提下进一步假设条件服从高斯分布(条件独立性假设是假设特征之间相互独立)，
从而采用高斯概率密度的值来表示P(Xj=xj|Y=ck),从而用多个高斯概率密度的乘积来表示连乘积P(Xj=xj|Y=ck)。

**Q. 朴素贝叶斯对非线性特征的拟合能力强吗，为什么？**
<br>朴素贝叶斯分类算法，在所有非线性分类算法中(knn/svm/nb)受限制最大，效果也不好。
主要原因在与朴素贝叶斯的连续特征分类算法是假定每个特征都是正态分布的前提下去估计概率，
而对于大部分非线性特征(比如典型的moon数据集)，其分布往往跟正态分布相差很远，所以分类的效果就很差。而对于那些离散的
或者是无规律分散的特征(比如circle数据集)，往往其分布是正态分布，所以效果就会不错。其他非线性算法由于不受特征
分布的影响，所以效果会比朴素贝叶斯更好。如下图示：
<div align=center><img src="https://github.com/ximitiejiang/machine_learning_algorithm/blob/master/demo/naive_bayes_continuous2.png"/></div>


---
## PART 7. decision tree classifier
**Q. CART决策树有什么优缺点？**
- CART优点：可同时适合于离散数据和连续数据，非线性拟合能力极强(可以跟svm媲美)，甚至很容易过拟合
- CART缺点：容易过拟合。改进过拟合的方法包括：采用随机森林



---
## PART 1x. PCA
**Q. PCA的作用和理论**
<br>PCA主成分分析，主要用来给特征降维，也就是找到方差最大的特征。
<br>PCA主成分分析过程:
- 计算每一列的均值x1mean, x2mean, x3mean, x4mean...
- 计算所有特征之间的协方差矩阵C(cov(X1,X1), cov(X1,X2), cov(X1,X3,...)。
```
C = (cov(X,X), cov(X,Y), cov(X,Z),
     cov(Y,X), cov(Y,Y), cov(Y,Z),
     cov(Z,X), cov(Z,Y), cov(Z,Z)) 
```
对于协方差矩阵种的值，也就是协方差值，如果值大于0，则两个特征正相关。如果值小于0，则两个特征负相关。如果值等于0，则两个特征相互独立

- 计算该协方差矩阵的特征值和特征向量：有两种方法，一种是特征值分解协方差矩阵，另一种是奇异值分解协方差矩阵。
其中基于特征值分解协方差矩阵就是传统线性代数的知识()
- 排序特征值，提取最大的k个特征值和特征向量




